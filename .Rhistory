names(x) <- tolower(names(x))
x
}) %>%
na.omit()
df %>%
select(where(is.numeric)) %>%
cor() %>%
round(digits = 2) %>%
corrplot(diag = F)
set.seed(42)
test_ind <- sample(
1:nrow(df),
floor( nrow(df)/10 ),
replace=FALSE
)
df_train <- df[-test_ind, ]
df_test  <- df[test_ind, ]
lm_fit <- lm(median_house_value ~ . -households, data = df_train)
rmse <- function(y, yhat) {
sqrt(mean((y - yhat)^2))
}
lm_predictions <- predict(lm_fit, newdata = df_test)
lm_RMSE <- rmse(df_test$median_house_value, lm_predictions)
lm_RMSE
rpart_fit <- rpart(median_house_value ~ . -households, data = df_train)
rpart_predictions <- predict(rpart_fit, newdata = df_test)
rpart.plot(rpart_fit, tweak = 0.8)
rpart_RMSE <- rmse(df_test$median_house_value, rpart_predictions)
rpart_RMSE
svm_fit <- svm(median_house_value ~ . -households, data = df_train, kernel = 'radial')
svm_predictions <- predict(svm_fit, newdata = df_test)
svm_RMSE <- rmse(df_test$median_house_value, svm_predictions)
svm_RMSE
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$hidden1 <- nn_linear(p, q1)
self$hidden2 <- nn_linear(q1, q2)
self$hidden3 <- nn_linear(q2, q3)
self$output <- nn_linear(q3,1)
self$activation <- nn_relu()
},
forward = function(x){
x %>%
self$hidden1() %>%
self$activation() %>%
self$hidden2() %>%
self$activation() %>%
self$hidden3() %>%
self$activation() %>%
self$output()
}
)
M <- model.matrix(median_house_value ~ 0 + . -households, data = df_train)
nnet_fit <- NNet %>%
setup(
loss = nn_mse_loss(),
optimizer = optim_adam,
metrics = list(
luz_metric_mse()
)
) %>%
set_hparams(
p = ncol(M), q1 = 32, q2 = 16, q3 = 8
) %>%
set_opt_hparams(
lr = 1e-2
) %>%
fit(
data = list(
model.matrix(median_house_value ~ 0 + . -households, data = df_train),
df_train %>% select(median_house_value) %>% as.matrix
),
valid_data = list(
model.matrix(median_house_value ~ 0 + . -households, data = df_test),
df_test %>% select(median_house_value) %>% as.matrix
),
epochs = 100,
dataloader_options = list(batch_size = 512, shuffle = TRUE),
verbose = TRUE # Change to TRUE while tuning. But, set to FALSE before submitting
)
plot(nnet_fit)
nnet_predictions <- predict(nnet_fit,
model.matrix(median_house_value ~ 0 + . -households,
data = df_test)) %>% as.array()
nn_RMSE <- rmse(df_test$median_house_value, nnet_predictions)
nn_RMSE
summary_table <- data.frame(
Model = c('Linear Regression', 'Decision Tree', 'SVM', 'Neural Network'),
RMSE = c(lm_RMSE, rpart_RMSE, svm_RMSE, nn_RMSE)
)
summary_table
df2 <- read.csv("data/spambase.csv") %>%
mutate(spam = as.factor(spam)) %>%
(\(x) {
names(x) <- tolower(names(x))
x
}) %>%
na.omit()
set.seed(42)
test_ind <- sample(
1:nrow(df2),
floor( nrow(df2)/10 ),
replace=FALSE
)
df2_train <- df2[-test_ind, ]
df2_test  <- df2[test_ind, ]
overview <- function(pred_class, true_class) {
accuracy <- mean(pred_class == true_class)
error <- 1 - accuracy
true_positives <- sum(pred_class == "1" & true_class == "1")
true_negatives <- sum(pred_class == "0" & true_class == "0")
false_positives <- sum(pred_class == "1" & true_class == "0")
false_negatives <- sum(pred_class == "0" & true_class == "1")
true_positive_rate <- true_positives / (true_positives + false_negatives)
false_positive_rate <- false_positives / (false_positives + true_negatives)
return(
data.frame(
accuracy = accuracy,
error = error,
true_positive_rate = true_positive_rate,
false_positive_rate = false_positive_rate
)
)
}
glm_fit <- glm(spam ~ ., data = df2_train, family = binomial())
glm_classes <- ifelse(predict(glm_fit, newdata = df2_test) > 0.5, 1, 0) %>% as.factor
overview(glm_classes, df2_test$spam)
rpart_classes <- rpart(spam ~ ., data = df2_train, method = 'class')
rpart.plot(rpart_classes)
rpart_classes <- ifelse(predict(rpart_classes, newdata = df2_test) > 0.5, 1, 0) %>% as.factor
overview(rpart_classes, df2_test$spam)
svm_fit <- svm(spam ~ . , data = df2_train, kernel = 'radial')
svm_classes <- predict(svm_fit, newdata = df2_test)
overview(svm_classes, df2_test$spam)
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$hidden1 <- nn_linear(p, q1)
self$hidden2 <- nn_linear(q1, q2)
self$hidden3 <- nn_linear(q2, q3)
self$output <- nn_linear(q3, 1)
self$activation <- nn_relu()
self$sigmoid <- nn_sigmoid()
},
forward = function(x){
x %>%
self$hidden1() %>%
self$activation() %>%
self$hidden2() %>%
self$activation() %>%
self$hidden3() %>%
self$activation() %>%
self$output() %>%
self$sigmoid()
}
)
M3 <- model.matrix(spam ~ 0 + ., data = df2_train)
M4 <- model.matrix(spam ~ 0 + ., data = df2_test)
nnet_fit <- NNet %>%
setup(
loss = nn_bce_loss(),
optimizer = optim_adam,
) %>%
set_hparams(
p = ncol(M3), q1 = 32, q2 = 16, q3 = 8
) %>%
set_opt_hparams(
lr = 1e-2
) %>%
fit(
data = list(
model.matrix(spam ~ 0 + ., data = df2_train),
(df2_train[["spam"]] %>% as.numeric() - 1) %>% as.matrix()
),
valid_data = list(
model.matrix(spam ~ 0 + ., data = df2_test),
(df2_test[["spam"]] %>% as.numeric() - 1) %>% as.matrix()
),
epochs = 100,
dataloader_options = list(batch_size = 512, shuffle = TRUE),
verbose = TRUE # Change to TRUE while tuning. But, set to FALSE before submitting
)
nnet_predictions <- ifelse(predict(nnet_fit,
model.matrix(spam ~ 0 + ., data = df2_test)) > 0.5, 1, 0)
list(glm_classes, rpart_classes, svm_classes, nnet_predictions) %>%
lapply(\(x) overview(x, df2_test$spam)) %>%
bind_rows() %>%
cbind(Model = c('Logistic Regression', 'Decision Tree', 'SVM', 'Neural Network')) %>%
select(Model, accuracy, error, true_positive_rate, false_positive_rate)
generate_three_spirals <- function(){
set.seed(42)
n <- 500
noise <- 0.2
t <- (1:n) / n * 2 * pi
x1 <- c(
t * (sin(t) + rnorm(n, 0, noise)),
t * (sin(t + 2 * pi/3) + rnorm(n, 0, noise)),
t * (sin(t + 4 * pi/3) + rnorm(n, 0, noise))
)
x2 <- c(
t * (cos(t) + rnorm(n, 0, noise)),
t * (cos(t + 2 * pi/3) + rnorm(n, 0, noise)),
t * (cos(t + 4 * pi/3) + rnorm(n, 0, noise))
)
y <- as.factor(
c(
rep(0, n),
rep(1, n),
rep(2, n)
)
)
return(tibble(x1=x1, x2=x2, y=y))
}
df3 <- generate_three_spirals()
plot(
df3$x1, df3$x2,
col = df3$y,
pch = 20
)
grid <- expand.grid(x1 = seq(-10, 10, length.out = 10), x2 = seq(-10, 10, length.out = 10))
df3_test <- tibble(grid)
rpart_fit <- rpart(y ~ df3$x1 + df3$x2, data = df3)
rpart_classes <- predict(rpart_fit, df3$y)
plot_decision_boundary <- function(predictions){
plot(
df3_test$x1, df3_test$x2,
col = predictions,
pch = 0
)
points(
df3$x1, df3$x2,
col = df3$y,
pch = 20
)
}
plot_decision_boundary(rpart_classes)
svm_fit <- svm(y ~ df3$x1 + df3$x2 , data = df3, type = 'C-classification')
svm_classes <- predict(svm_fit, newdata = df3_test)
plot_decision_boundary(svm_classes)
NN1 <- nn_module(
initialize = function(p, q1, o){
self$hidden1 <- nn_linear(p, q1)
self$output <- nn_linear(q1, o)
self$activation <- nn_relu()
},
forward = function(x){
x %>%
self$hidden1() %>%
self$activation() %>%
self$output()
}
)
fit_1 <- NN1 %>%
setup(
loss = nn_mse_loss(),
optimizer = optim_adam
) %>%
set_hparams(
p = ncol(df3), q1 = 10, o = 3
) %>%
set_opt_hparams(
lr = 1e-2
) %>%
fit(
data = list(
df3 %>% select(x1, x2) %>% as.matrix,
df3$y %>% as.integer
),
epochs = 50,
dataloader_options = list(batch_size = 200, shuffle = TRUE),
verbose = FALSE
)
data = list(
df3 %>% select(x1, x2) %>% as.matrix,
df3$y %>% as.integer
)
data
data
data = list(
df3 %>% select(x1, x2) %>% as.matrix,
df3$y %>% as.integer
)
data
data %>% tail()
grid <- expand.grid(x1=seq(-10,10,length.out=100), x2=seq(-10,10,length.out=100))
df3_test <- tibble(grid)
df3_test
df3
rpart_fit <- rpart(y ~ df3$x1 + df3$x2, data = df3)
rpart_classes <- predict(rpart_fit, df3$y)
rpart.plot(rpart_classes)
rpart.plot(rpart_fit)
rpart_classes <- predict(rpart_fit, df3$y)
plot_decision_boundary <- function(predictions){
plot(
df3_test$x1, df3_test$x2,
col = predictions,
pch = 0
)
points(
df3$x1, df3$x2,
col = df3$y,
pch = 20
)
}
plot_decision_boundary(rpart_classes)
generate_three_spirals <- function(){
set.seed(42)
n <- 500
noise <- 0.2
t <- (1:n) / n * 2 * pi
x1 <- c(
t * (sin(t) + rnorm(n, 0, noise)),
t * (sin(t + 2 * pi/3) + rnorm(n, 0, noise)),
t * (sin(t + 4 * pi/3) + rnorm(n, 0, noise))
)
x2 <- c(
t * (cos(t) + rnorm(n, 0, noise)),
t * (cos(t + 2 * pi/3) + rnorm(n, 0, noise)),
t * (cos(t + 4 * pi/3) + rnorm(n, 0, noise))
)
y <- as.factor(
c(
rep(0, n),
rep(1, n),
rep(2, n)
)
)
return(tibble(x1=x1, x2=x2, y=y))
}
df3 <- generate_three_spirals()
df3
plot(
df3$x1, df3$x2,
col = df3$y,
pch = 20
)
rpart_fit <- rpart(y ~ df3$x1 + df3$x2, data = df3)
rpart_classes <- predict(rpart_fit, df3$y)
plot_decision_boundary <- function(predictions){
plot(
df3_test$x1, df3_test$x2,
col = predictions,
pch = 0
)
points(
df3$x1, df3$x2,
col = df3$y,
pch = 20
)
}
plot_decision_boundary(rpart_classes)
svm_fit <- svm(y ~ df3_test$x1 + df3_test$x2 , data = df3_test, type = 'C-classification')
svm_fit <- svm(y ~ df3$x1 + df3$x2 , data = df3, type = 'C-classification')
svm_classes <- predict(svm_fit, newdata = df3_test)
svm_fit <- svm(y ~ df3$x1 + df3$x2 , data = df3, type = 'C-classification')
svm_classes <- predict(svm_fit, newdata = df3_test)
svm_classes <- predict(svm_fit, newdata = df3)
plot_decision_boundary(svm_classes)
svm_classes <- predict(svm_fit, newdata = df3_test)
svm_fit <- svm(y ~ df3$x1 + df3$x2 , data = df3, type = 'C-classification')
svm_classes <- predict(svm_fit, newdata = df3_test)
svm_fit <- svm(y ~ df3$x1 + df3$x2 , data = df3, type = 'C-classification', kernel = 'radial')
svm_classes <- predict(svm_fit, newdata = df3)
plot_decision_boundary(svm_classes)
svm_classes <- predict(svm_fit, newdata = df3_test)
svm_classes <- predict(svm_fit, newdata = df3)
svm_classes
grid <- expand.grid(x1=seq(-10,10,length.out=100), x2=seq(-10,10,length.out=100),
y=seq(-10,10,lenght.out-100))
grid <- expand.grid(x1=seq(-10,10,length.out=100), x2=seq(-10,10,length.out=100),
y=seq(-10,10,length.out-100))
grid <- expand.grid(x1=seq(-10,10,length.out=100), x2=seq(-10,10,length.out=100),
y=seq(-10,10,length.out=100))
df3_test <- tibble(grid)
df3_test
grid <- expand.grid(x1=seq(-10,10,length.out=100), x2=seq(-10,10,length.out=100))
df3_test <- tibble(grid)
grid <- expand.grid(x1=seq(-10,10,length.out=100), x2=seq(-10,10,length.out=100))
df3_test <- tibble(grid)
rpart_fit <- rpart(y ~ df3$x1 + df3$x2, data = df3)
rpart_classes <- predict(rpart_fit, df3$y)
rpart_fit <- rpart(y ~ df3$x1 + df3$x2, data = df3)
rpart_fit
rpart_classes <- predict(rpart_fit, df3_test$y)
rpart_classes <- predict(rpart_fit, df3_test)
rpart_fit <- rpart(y ~ df3$x1 + df3$x2, data = df3)
rpart_classes <- predict(rpart_fit, df3_test)
rpart_classes
plot_decision_boundary <- function(predictions){
plot(
df3_test$x1, df3_test$x2,
col = predictions,
pch = 0
)
points(
df3$x1, df3$x2,
col = df3$y,
pch = 20
)
}
plot_decision_boundary(rpart_classes)
rpart_classes <- predict(rpart_fit, df3_test, type = 'prob')
rpart_classes
rpart_fit <- rpart(y ~ df3$x1 + df3$x2, data = df3)
rpart_classes <- predict(rpart_fit, df3_test, type = 'prob')
rpart_classes <- ifelse(rpart_classes[, "0"] > rpart_classes[, "1"] & rpart_classes[, "0"] > rpart_classes[, "2"], "0",
ifelse(rpart_classes[, "1"] > rpart_classes[, "0"] & rpart_classes[, "1"] > rpart_classes[, "2"], "1",
"2"))
rpart_classes
plot_decision_boundary <- function(predictions){
plot(
df3_test$x1, df3_test$x2,
col = predictions,
pch = 0
)
points(
df3$x1, df3$x2,
col = df3$y,
pch = 20
)
}
plot_decision_boundary(rpart_classes)
?predict()
rpart_fit <- rpart(y ~ df3$x1 + df3$x2, data = df3)
rpart_classes <- predict(rpart_fit, df3_test, type = 'class')
rpart_classes
rpart_classes <- predict(rpart_fit, df3_test, type = 'class')
generate_three_spirals <- function(){
set.seed(42)
n <- 500
noise <- 0.2
t <- (1:n) / n * 2 * pi
x1 <- c(
t * (sin(t) + rnorm(n, 0, noise)),
t * (sin(t + 2 * pi/3) + rnorm(n, 0, noise)),
t * (sin(t + 4 * pi/3) + rnorm(n, 0, noise))
)
x2 <- c(
t * (cos(t) + rnorm(n, 0, noise)),
t * (cos(t + 2 * pi/3) + rnorm(n, 0, noise)),
t * (cos(t + 4 * pi/3) + rnorm(n, 0, noise))
)
y <- as.factor(
c(
rep(0, n),
rep(1, n),
rep(2, n)
)
)
return(tibble(x1=x1, x2=x2, y=y))
}
df3 <- generate_three_spirals()
plot(
df3$x1, df3$x2,
col = df3$y,
pch = 20
)
rpart_fit <- rpart(y ~ df3$x1 + df3$x2, data = df3)
rpart_classes <- predict(rpart_fit, df3_test, type = 'class')
plot_decision_boundary <- function(predictions){
plot(
df3_test$x1, df3_test$x2,
col = predictions,
pch = 0
)
points(
df3$x1, df3$x2,
col = df3$y,
pch = 20
)
}
plot_decision_boundary(rpart_classes)
svm_fit <- svm(y ~ df3$x1 + df3$x2 , data = df3, type = 'C-classification', kernel = 'radial')
svm_classes <- predict(svm_fit, newdata = df3)
plot_decision_boundary(svm_classes)
svm_fit <- svm(y ~ df3$x1 + df3$x2 , data = df3, type = 'C-classification', kernel = 'radial')
svm_classes <- predict(svm_fit, newdata = df3)
plot_decision_boundary(svm_classes)
grid <- expand.grid(x1=seq(-10,10,length.out=10), x2=seq(-10,10,length.out=10))
df3_test <- tibble(grid)
rpart_fit <- rpart(y ~ df3$x1 + df3$x2, data = df3)
rpart_classes <- predict(rpart_fit, df3_test, type = 'class')
plot_decision_boundary <- function(predictions){
plot(
df3_test$x1, df3_test$x2,
col = predictions,
pch = 0
)
points(
df3$x1, df3$x2,
col = df3$y,
pch = 20
)
}
plot_decision_boundary(rpart_classes)
grid <- expand.grid(x1=seq(-10,10,length.out=100), x2=seq(-10,10,length.out=100))
df3_test <- tibble(grid)
rpart_fit <- rpart(y ~ df3$x1 + df3$x2, data = df3)
rpart_classes <- predict(rpart_fit, df3_test, type = 'class')
plot_decision_boundary <- function(predictions){
plot(
df3_test$x1, df3_test$x2,
col = predictions,
pch = 0
)
points(
df3$x1, df3$x2,
col = df3$y,
pch = 20
)
}
plot_decision_boundary(rpart_classes)
