---
title: "Homework 5"
author: "Brady Miller"
toc: true
title-block-banner: true
title-block-style: default
execute: 
  freeze: true
  cache: true
format:
  # html: # comment this line to get pdf
  pdf: 
    fig-width: 7
    fig-height: 7
---

[Link to the Github repository](https://github.com/psu-stat380/hw-5)

---

::: {.callout-important style="font-size: 0.8em;"}
## Due: Wed, Apr 19, 2023 @ 11:59pm

Please read the instructions carefully before submitting your assignment.

1. This assignment requires you to only upload a `PDF` file on Canvas
1. Don't collapse any code cells before submitting. 
1. Remember to make sure all your code output is rendered properly before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter before submitting your assignment ⚠️
:::


In this assignment, we will explore decision trees, support vector machines and neural networks for classification and regression. The assignment is designed to test your ability to fit and analyze these models with different configurations and compare their performance.

We will need the following packages:


```{R, message=FALSE, warning=FALSE, results='hide'}
packages <- c(
  "dplyr", 
  "readr", 
  "tidyr", 
  "purrr", 
  "broom",
  "magrittr",
  "corrplot",
  "caret",
  "rpart",
  "rpart.plot",
  "e1071",
  "torch", 
  "luz",
  "tidyverse"
)

renv::install(packages)
sapply(packages, require, character.only=T)
```

<br><br><br><br>
---

## Question 1
::: {.callout-tip}
## 60 points
Prediction of Median House prices
:::

###### 1.1 (2.5 points)


The `data` folder contains the `housing.csv` dataset which contains housing prices in California from the 1990 California census. The objective is to predict the median house price for California districts based on various features.


Read the data file as a tibble in R. Preprocess the data such that:

1. the variables are of the right data type, e.g., categorical variables are encoded as factors
2. all column names to lower case for consistency
3. Any observations with missing values are dropped

```{R}
path <- "data/housing.csv"

df <- read.csv(path) %>%
    mutate_if(\(x) is.character(x), as.factor) %>%
    (\(x) {
        names(x) <- tolower(names(x))
        x
    }) %>%
  na.omit()
```

---

###### 1.2 (2.5 points)

Visualize the correlation matrix of all numeric columns in `df` using `corrplot()`

```{R}
df %>%
  select(where(is.numeric)) %>%
  cor() %>%
  round(digits = 2) %>%
  corrplot(diag = F)
```

---

###### 1.3 (5 points)

Split the data `df` into `df_train` and `df_split` using `test_ind` in the code below:

```{R}
set.seed(42)
test_ind <- sample(
  1:nrow(df), 
  floor( nrow(df)/10 ),
  replace=FALSE
)

df_train <- df[-test_ind, ]
df_test  <- df[test_ind, ]
```

---

###### 1.4 (5 points)

Fit a linear regression model to predict the `median_house_value` :

* `latitude`
* `longitude`
* `housing_median_age`
* `total_rooms`
* `total_bedrooms`
* `population`
* `median_income`
* `ocean_proximity`

Interpret the coefficients and summarize your results. 

```{R}
lm_fit <- lm(median_house_value ~ . -households, data = df_train)
```

---

###### 1.5 (5 points)

Complete the `rmse` function for computing the Root Mean-Squared Error between the true `y` and the predicted `yhat`, and use it to compute the RMSE for the regression model on `df_test`

```{R}
rmse <- function(y, yhat) {
  sqrt(mean((y - yhat)^2))
}

lm_predictions <- predict(lm_fit, newdata = df_test)

lm_RMSE <- rmse(df_test$median_house_value, lm_predictions)
lm_RMSE
```
The root mean square error on the test set is 68,339.82

###### 1.6 (5 points)

Fit a decision tree model to predict the `median_house_value` using the same predictors as in 1.4. Use the `rpart()` function.

```{R}
rpart_fit <- rpart(median_house_value ~ . -households, data = df_train)
rpart_predictions <- predict(rpart_fit, newdata = df_test)
```


Visualize the decision tree using the `rpart.plot()` function. 

```{R}
rpart.plot(rpart_fit, tweak = 0.8)
```


Report the root mean squared error on the test set.

```{R}
rpart_RMSE <- rmse(df_test$median_house_value, rpart_predictions)
rpart_RMSE
```
The root mean square error on the test set is 75,876.87

---

###### 1.7 (5 points)

Fit a support vector machine model to predict the `median_house_value` using the same predictors as in 1.4. Use the `svm()` function and use any kernel of your choice. Report the root mean squared error on the test set.

```{R}
svm_fit <- svm(median_house_value ~ . -households, data = df_train, kernel = 'radial')
svm_predictions <- predict(svm_fit, newdata = df_test)
svm_RMSE <- rmse(df_test$median_house_value, svm_predictions)
svm_RMSE
```
The root mean square error is 56,678.84

---

###### 1.8 (25 points)

Initialize a neural network model architecture:

```{R}
NNet <- nn_module(
    initialize = function(p, q1, q2, q3){
        self$hidden1 <- nn_linear(p, q1)
        self$hidden2 <- nn_linear(q1, q2)
        self$hidden3 <- nn_linear(q2, q3)
        self$output <- nn_linear(q3,1)
        self$activation <- nn_relu()
    },
    forward = function(x){
      x %>%
        self$hidden1() %>%
        self$activation() %>%
        self$hidden2() %>%
        self$activation() %>%
        self$hidden3() %>%
        self$activation() %>%
        self$output() 
    }
)
```


Fit a neural network model to predict the `median_house_value` using the same predictors as in 1.4. Use the `model.matrix` function to create the covariate matrix and `luz` package for fitting the network with $32, 16, 8$ nodes in each of the three hidden layers. 

```{R}
M <- model.matrix(median_house_value ~ 0 + . -households, data = df_train)

nnet_fit <- NNet %>% 
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam,
    metrics = list(
      luz_metric_mse()
    )
  ) %>%
  set_hparams(
    p = ncol(M), q1 = 32, q2 = 16, q3 = 8
  ) %>%
  set_opt_hparams(
    lr = 0.001
  ) %>%
  fit(
    data = list(
            model.matrix(median_house_value ~ 0 + . -households, data = df_train),
            df_train %>% select(median_house_value) %>% as.matrix
        ),
        valid_data = list(
            model.matrix(median_house_value ~ 0 + . -households, data = df_test),
            df_test %>% select(median_house_value) %>% as.matrix
        ),
    epochs = 100,
    dataloader_options = list(batch_size = 512, shuffle = TRUE),
    verbose = TRUE # Change to TRUE while tuning. But, set to FALSE before submitting
  )
```

Plot the results of the training and validation loss and accuracy.

```{R}
plot(nnet_fit)
```


Report the root mean squared error on the test set.


```{R}
nnet_predictions <- predict(nnet_fit, 
                            model.matrix(median_house_value ~ 0 + . -households,
                                         data = df_test)) %>% as.array()

nn_RMSE <- rmse(df_test$median_house_value, nnet_predictions)
nn_RMSE
```

::: {.callout-warning}
Remember to use the `as_array()` function to convert the predictions to a vector
of numbers before computing the RMSE with `rmse()`
:::

---

###### 1.9 (5 points)

Summarize your results in a table comparing the RMSE for the different models. 
Which model performed best? Why do you think that is?

```{R}
summary_table <- data.frame(
  Model = c('Linear Regression', 'Decision Tree', 'SVM', 'Neural Network'),
  RMSE = c(lm_RMSE, rpart_RMSE, svm_RMSE, nn_RMSE)
)
summary_table
```

The model that performed best was the svm model. I believe this is because the
data is non-linear so a good model would require the fit of the model to be able
to 'flow' with tendencies of the data. SVM is good for handling non-linear data
through the different kernels that can be used for its model. 


<br><br><br><br>
<br><br><br><br>
---

## Question 2
::: {.callout-tip}
## 50 points
Spam email classification
:::

The `data` folder contains the `spam.csv` dataset. This dataset contains features extracted from a collection of spam and non-spam emails. The objective is to classify the emails as spam or non-spam.

---

###### 2.1 (2.5 points)

Read the data file as a tibble in R. Preprocess the data such that:

1. the variables are of the right data type, e.g., categorical variables are encoded as factors
2. all column names to lower case for consistency
3. Any observations with missing values are dropped

```{R}
df2 <- read.csv("data/spambase.csv") %>%
    mutate(spam = as.factor(spam)) %>%
    (\(x) {
        names(x) <- tolower(names(x))
        x
    }) %>%
  na.omit()
```

---

###### 2.2 (2.5 points)

Split the data `df` into `df_train` and `df_split` using `test_ind` in the code below:

```{R}
set.seed(42)
test_ind <- sample(
  1:nrow(df2), 
  floor( nrow(df2)/10 ),
  replace=FALSE
)

df2_train <- df2[-test_ind, ]
df2_test  <- df2[test_ind, ]
```

Complete the `overview` function which returns a data frame with the following columns: `accuracy`, `error`, `false positive rate`, `true positive rate`, between the true `true_class` and the predicted `pred_class` for any classification model.

```{R}
overview <- function(pred_class, true_class) {
  accuracy <- mean(pred_class == true_class)
  error <- 1 - accuracy
  true_positives <- sum(pred_class == "1" & true_class == "1")
  true_negatives <- sum(pred_class == "0" & true_class == "0")
  false_positives <- sum(pred_class == "1" & true_class == "0")
  false_negatives <- sum(pred_class == "0" & true_class == "1")
  true_positive_rate <- true_positives / (true_positives + false_negatives)
  false_positive_rate <- false_positives / (false_positives + true_negatives)
  return(
    data.frame(
      accuracy = accuracy,
      error = error,
      true_positive_rate = true_positive_rate,
      false_positive_rate = false_positive_rate
    )
  )
}
```


---

###### 2.3 (5 points)

Fit a logistic regression model to predict the `spam` variable using the remaining predictors. Report the prediction accuracy on the test set.

```{R}
glm_fit <- glm(spam ~ ., data = df2_train, family = binomial())
glm_classes <- ifelse(predict(glm_fit, newdata = df2_test) > 0.5, 1, 0) %>% as.factor
overview(glm_classes, df2_test$spam)
```
The accuracy on the test set is about 91%

---

###### 2.4 (5 points)

Fit a decision tree model to predict the `spam` variable using the remaining predictors. Use the `rpart()` function and set the `method` argument to `"class"`. 

```{R}
rpart_classes <- rpart(spam ~ ., data = df2_train, method = 'class')
```

Visualize the decision tree using the `rpart.plot()` function. 

```{R}
rpart.plot(rpart_classes)
```

Report the prediction accuracy on the test set.

```{R}
rpart_classes <- ifelse(predict(rpart_classes, newdata = df2_test) > 0.5, 1, 0) %>% as.factor
overview(rpart_classes, df2_test$spam)
```
The accuracy on the test set is 0.5

---

###### 2.5 (5 points)

Fit a support vector machine model to predict the `spam` variable using the remaining predictors. Use the `svm()` function and use any kernel of your choice. Remember to set the `type` argument to `"C-classification"` **if you haven't** already converted `spam` to be of type `factor`.


```{R}
svm_fit <- svm(spam ~ . , data = df2_train, kernel = 'radial')
```

Report the prediction accuracy on the test set.

```{R}
svm_classes <- predict(svm_fit, newdata = df2_test)
overview(svm_classes, df2_test$spam)
```
The accuracy on the test set is about 92.4%

---

###### 2.6 (25 points)

Using the same neural network architecture as in 1.9, fit a neural network model to predict the `spam` variable using the remaining predictors. 

::: {.callout-warning}
## Classification vs. Regression

Note that the neural network in **Q 1.9** was a regression model. You will need to modify the neural network architecture to be a classification model by changing the output layer to have a single node with a sigmoid activation function.
:::

```{r}
NNet <- nn_module(
    initialize = function(p, q1, q2, q3){
        self$hidden1 <- nn_linear(p, q1)
        self$hidden2 <- nn_linear(q1, q2)
        self$hidden3 <- nn_linear(q2, q3)
        self$output <- nn_linear(q3, 1)
        self$activation <- nn_relu()
        self$sigmoid <- nn_sigmoid()
    },
    forward = function(x){
      x %>%
        self$hidden1() %>%
        self$activation() %>%
        self$hidden2() %>%
        self$activation() %>%
        self$hidden3() %>%
        self$activation() %>%
        self$output() %>%
        self$sigmoid()
    }
)
```

Use the `model.matrix` function to create the covariate matrix and `luz` package for fitting the network with $32, 16, 8$ nodes in each of the three hidden layers. 

```{R}
M3 <- model.matrix(spam ~ 0 + ., data = df2_train)
M4 <- model.matrix(spam ~ 0 + ., data = df2_test)

nnet_fit <- NNet %>%
  setup(
    loss = nn_bce_loss(),
    optimizer = optim_adam,
  ) %>%
  set_hparams(
    p = ncol(M3), q1 = 32, q2 = 16, q3 = 8
  ) %>%
  set_opt_hparams(
     lr = 0.001
  ) %>%
  fit(
data = list(
            model.matrix(spam ~ 0 + ., data = df2_train),
            (df2_train[["spam"]] %>% as.numeric() - 1) %>% as.matrix()
        ),
        valid_data = list(
            model.matrix(spam ~ 0 + ., data = df2_test),
            (df2_test[["spam"]] %>% as.numeric() - 1) %>% as.matrix()
        ),
    epochs = 100,
    dataloader_options = list(batch_size = 512, shuffle = TRUE),
    verbose = TRUE # Change to TRUE while tuning. But, set to FALSE before submitting
  )


nnet_predictions <- ifelse(predict(nnet_fit, 
                       model.matrix(spam ~ 0 + ., data = df2_test)) > 0.5, 1, 0)
```

---

###### 2.7 (5 points)

Summarize your results in a table comparing the accuracy metrics for the different models. 

```{R}
list(glm_classes, rpart_classes, svm_classes, nnet_predictions) %>%
    lapply(\(x) overview(x, df2_test$spam)) %>%
    bind_rows() %>%
  cbind(Model = c('Logistic Regression', 'Decision Tree', 'SVM', 'Neural Network')) %>%
  select(Model, accuracy, error, true_positive_rate, false_positive_rate)
```

If you were to choose a model to classify spam emails, which model would you choose? Think about the context of the problem and the cost of false positives and false negatives.

In terms of classifying spam emails, false positives would be incorrectly classifying a real/non-spam email as spam, while false negatives would be when the computer doesn't recognize an email as spam, and instead classifies it as legitimate. This can lead to someone accidentally opening contents within the misclassified, actual spam email, causing phishing or malware attacks or other threats against their privacy and security. Based on this, the model I would choose to classify spam emails would be the neural network model. False negative rate is obviously the most threatening thing for a email user, so we want to prioritize that rate. True positive rate is calculated by dividing the true positive rate by the sum of the true positives and false negatives. So to get the false negative rate you can just do 1 - true positive rate. Doing this, you can see that the neural network would have the lowest false negative rate. While it doesn't have the best accuracy and error of the models, its close to the top performers in those categories and it has the best true positive rate, so it is best at correctly identifying spam. Thus, the neural network model would be the best for identifying spam.


<br><br><br><br>
<br><br><br><br>
---

## Question 3
::: {.callout-tip}
## 60 points

Three spirals classification

:::

To better illustrate the power of depth in neural networks, we will use a toy dataset called the "Three Spirals" data. This dataset consists of two intertwined spirals, making it challenging for shallow models to classify the data accurately. 

::: {.callout-warning}
## This is a multi-class classification problem
:::

The dataset can be generated using the provided R code below:

```{R}
generate_three_spirals <- function(){
  set.seed(42)
  n <- 500
  noise <- 0.2
  t <- (1:n) / n * 2 * pi
  x1 <- c(
      t * (sin(t) + rnorm(n, 0, noise)),
      t * (sin(t + 2 * pi/3) + rnorm(n, 0, noise)),
      t * (sin(t + 4 * pi/3) + rnorm(n, 0, noise))
    )
  x2 <- c(
      t * (cos(t) + rnorm(n, 0, noise)),
      t * (cos(t + 2 * pi/3) + rnorm(n, 0, noise)),
      t * (cos(t + 4 * pi/3) + rnorm(n, 0, noise))
    )
  y <- as.factor(
    c(
      rep(0, n), 
      rep(1, n), 
      rep(2, n)
    )
  )
  return(tibble(x1=x1, x2=x2, y=y))
}
```

---

###### 3.1 (5 points)

Generate the three spirals dataset using the code above. Plot $x_1$ vs $x_2$ and use the `y` variable to color the points. 


```{R}
df3 <- generate_three_spirals()

plot(
  df3$x1, df3$x2,
  col = df3$y,
  pch = 20
)
```

Define a grid of $100$ points from $-10$ to $10$ in both $x_1$ and $x_2$ using the `expand.grid()`. Save it as a tibble called `df_test`. 

```{R}
grid <- expand.grid(x1=seq(-10,10,length.out=100), x2=seq(-10,10,length.out=100))
df3_test <- as_tibble(grid)
```

---

###### 3.2 (10 points)

Fit a classification tree model to predict the `y` variable using the `x1` and `x2` predictors, and plot the decision boundary. 

```{R}
rpart_fit <- rpart(y ~ x1 + x2, data = df3, method = 'class')
rpart_classes <- predict(rpart_fit, df3_test, type = 'class')
```

Plot the decision boundary using the following function:

```{R}
plot_decision_boundary <- function(predictions){
  plot(
    df3_test$x1, df3_test$x2, 
    col = predictions,
    pch = 0
  )
  points(
    df3$x1, df3$x2,
    col = df3$y,
    pch = 20
  )
}
```

```{R}
plot_decision_boundary(rpart_classes)
```

---

###### 3.3 (10 points)

Fit a support vector machine model to predict the `y` variable using the `x1` and `x2` predictors. Use the `svm()` function and use any kernel of your choice. Remember to set the `type` argument to `"C-classification"` **if you haven't** converted `y` to be of type `factor`.

```{R}
svm_fit <- svm(y ~ x1 + x2 , data = df3, type = 'C-classification', kernel = 'radial', method = 'class')
svm_classes <- predict(svm_fit, newdata = df3_test, type = 'class')
plot_decision_boundary(svm_classes)
```

---

::: {.callout-warning}
## Instructions

For the next questions, you will need to fit a series of neural networks. In all cases, you can:

* set the number of units in each hidden layer to 10 
* set the output dimension `o` to 3 (remember this is multinomial classification)
* use the appropriate loss function for the problem (**not `nn_bce_loss`**)
* set the number of epochs to $50$
* fit the model using the `luz` package

You can use any optimizer of your choice, but you **will need to tune the learning rate for each problem**.
:::


###### 3.4 (10 points)

Fit a neural network with **1 hidden layer** to predict the `y` variable using the `x1` and `x2` predictors.

```{R}
NN1 <- nn_module(
  initialize = function(p, q1, o){
    self$hidden1 <- nn_linear(p, q1)
    self$output <- nn_linear(q1, o)
    self$activation <- nn_relu()
  },
  forward = function(x){
    x %>% 
      self$hidden1() %>% 
      self$activation() %>% 
      self$output()
  }
)

fit_1 <- NN1 %>% 
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam
  ) %>%
  set_hparams(
   p = ncol(df3_test), q1 = 10, o = 3
  ) %>%
  set_opt_hparams(
    lr = 0.001
  ) %>%
  fit(
    data = list(
      df3 %>% select(x1, x2) %>% as.matrix,
      df3$y %>% as.integer
    ),
    epochs = 50,
    dataloader_options = list(batch_size = 100, shuffle = TRUE),
    verbose = FALSE
  )
```

In order to generate the class predictions, you will need to use the `predict()` function as follows

```{R}
test_matrix <- df3_test %>% select(x1, x2) %>% as.matrix

fit_1_predictions <- predict(fit_1, test_matrix) %>% 
  torch_argmax(2) %>% 
  as.integer()
```

Plot the results using the `plot_decision_boundary()` function

```{r}
plot_decision_boundary(fit_1_predictions)
```

---

###### 3.5 (10 points)

Fit a neural network with **0 hidden layers** to predict the `y` variable using the `x1` and `x2` predictors.

```{R}
NN0 <- nn_module(
  initialize = function(p, o){
    self$l <- nn_linear(p,o)
  },
  forward = function(x){
    x %>% 
      self$l()
  }
)

fit_0 <- NN0 %>% 
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam
  ) %>%
  set_hparams(p = ncol(df3_test), o = 3) %>%
  set_opt_hparams(lr=0.001) %>%
  fit(
    data = list(
      df3 %>% select(x1, x2) %>% as.matrix,
      df3$y 
    ),
    epochs = 50,
    dataloader_options = list(batch_size = 100, shuffle = TRUE),
    verbose = FALSE
  )


test_matrix <- df3_test %>% select(x1, x2) %>% as.matrix

fit_0_predictions <- predict(fit_0, test_matrix) %>% 
  torch_argmax(2) %>% 
  as.integer()
```

Plot the results using the `plot_decision_boundary()` function.

```{r}
plot_decision_boundary(fit_0_predictions)
```

---


###### 3.6 (10 points)

Fit a neural network with **3 hidden layers** to predict the `y` variable using the `x1` and `x2` predictors.

```{r}
NN3 <- nn_module(
  initialize = function(p, q1, q2, o){
    self$hidden1 <- nn_linear(p, q1)
    self$hidden2 <- nn_linear(q1, q2)
    self$hidden3 <- nn_linear(q2, o)
    self$activation <- nn_relu()
  },
  forward = function(x){
    x %>% 
      self$hidden1() %>%
      self$activation() %>%
      self$hidden2() %>%
      self$activation() %>%
      self$hidden3() 
  }
)

fit_2 <- NN3 %>% 
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam
    ) %>%
  set_hparams(p = ncol(df3_test), q1 = 10, q2 = 10, o = 3) %>%
  set_opt_hparams(lr = 0.001) %>%
  fit(
    data = list(
      df3 %>% select(x1, x2) %>% as.matrix,
      df3$y 
    ),
    epochs = 50,
    dataloader_options = list(batch_size = 100, shuffle = TRUE),
    verbose = FALSE
  )

test_matrix <- df3_test %>% select(x1, x2) %>% as.matrix

fit_2_predictions <- predict(fit_2, test_matrix) %>% 
  torch_argmax(2) %>% 
  as.integer()
```

Plot the results using the `plot_decision_boundary()` function.

```{r}
plot_decision_boundary(fit_2_predictions)
```


---

###### 3.7 (5 points)

What are the differences between the models? How do the decision boundaries change as the number of hidden layers increases?

The difference between the models is how well each one classifies the points 
of each colored spiral away, especially towards the outer parts of the image. As
the number of hidden layers increases, the decision boundary gets more precise
and is better at classifying the points of the spirals. For example, in the 
neural network with 0 hidden layers it breaks the image up into 3 distinct 
sections, divided by straight lines. This doesn't match the tendency or curve 
of the data, and does a very poor job classifying the points. While each section 
may contain may points of the correct color, it also incorrectly classifies many
points of the other 2 colors. In the neural network with one hidden layer,
the classification of the points could still be considered poor, but is better 
than the neural network with 0 hidden layers. This neural network also has those 
3 sections that it divides most of then image into but at the center, where the 
spirals converge, it starts to create little sections of individual colors that 
do a better job at classifying those points near the center. Finally, for the 
neural network with 3 hidden layers, this one does a great job at establishing
the decision boundary as it follows the flow of each spiral all the way from the
center to the end of each one. While there are still a few misclassifications, 
the amount it has could be considered acceptable as its a very small amount and 
you wouldn't want to necessarily add more as it may lead to overfitting.

---


:::{.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br>
<br><br><br><br>
---



::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::